# invest-rag

**Production-Structured RAG System with Measurable Retrieval Performance**

Vector Retrieval + LLM Reranking  
Document-Level & Chunk-Level Benchmarking  
Strict Citation-Grounded Generation  

---
## ğŸš€ Run the Full Pipeline (CLI)

You can run the full retrieval + rerank pipeline from the command line:

```bash
python src/app/cli.py \
    --query "Which company discusses ecosystem lock-in as a competitive moat?" \
    --top_k 5 \
    --rerank
```
---

## Overview

`invest-rag` is a reproducible Retrieval-Augmented Generation (RAG) system built over multiple SEC 10-K filings (2024).

The goal of this project is not just to implement RAG, but to:

- Quantitatively measure retrieval quality (Recall@k, MRR)
- Compare vector-only retrieval vs LLM-based reranking
- Evaluate document-level vs chunk-level behavior
- Enforce strict citation grounding
- Demonstrate safe out-of-distribution handling

All components (retrieval, rerank, generation, evaluation) are modular and independently testable.

---

## Dataset

Indexed SEC 10-K filings (2024):

- Apple
- NVIDIA
- Microsoft
- Meta
- AMD

Granularity:

- **Document-level:** SEC section (e.g., `apple_2024_item_1_business`)
- **Chunk-level:** ~4000 deterministic semantic chunks

Each chunk contains:

- chunk_id
- doc_id
- company
- year
- section
- content

---

## System Architecture

                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   SEC 10-K Files   â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   Chunking Layer   â”‚
                â”‚  (~4000 chunks)    â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   Embedding Model  â”‚
                â”‚ (L2 Normalization) â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚     FAISS Index    â”‚
                â”‚   (Inner Product)  â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   Vector Retrieval â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   LLM Reranker     â”‚ (optional)
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  Grounded Generate â”‚
                â”‚ + Citation Check   â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

---

## Evaluation Results

Evaluation benchmark: 40 curated cross-company questions.

### Document-Level Performance

| k  | Vector R@k | Vector MRR | Rerank R@k | Rerank MRR | Î”Recall | Î”MRR |
|----|------------|------------|------------|------------|---------|------|
| 1  | 0.750 | 0.7500 | 0.875 | 0.8750 | +0.125 | +0.1250 |
| 3  | 0.850 | 0.8000 | 0.925 | 0.9000 | +0.075 | +0.1000 |
| 5  | 0.900 | 0.8188 | 0.950 | 0.9125 | +0.050 | +0.0938 |
| 10 | 0.975 | 0.8375 | 0.975 | 0.9208 | +0.000 | +0.0833 |

Key insight:
- High recall from vector retriever (R@10 = 0.975)
- Significant top-1 improvement from LLM reranking
- Rerank improves ranking quality without increasing candidate depth

---

### Chunk-Level Performance

| k  | Vector R@k | Vector MRR | Rerank R@k | Rerank MRR | Î”Recall | Î”MRR |
|----|------------|------------|------------|------------|---------|------|
| 1  | 0.150 | 0.1500 | 0.350 | 0.3500 | +0.20 | +0.2000 |
| 3  | 0.425 | 0.2792 | 0.525 | 0.4375 | +0.10 | +0.1583 |
| 5  | 0.525 | 0.3017 | 0.575 | 0.4488 | +0.05 | +0.1471 |
| 10 | 0.625 | 0.3135 | 0.625 | 0.4544 | +0.00 | +0.1408 |

Key insight:
- Chunk-level retrieval is substantially harder.
- Reranking provides large MRR gains, meaning it promotes the correct evidence chunk toward the top.
- Improvement is primarily ranking correction, not recall expansion.

---

## Safety: Out-of-Distribution Handling

Query example:
"Explain Risk Factors of the company Tesla."

Tesla is not indexed.

System behavior:

- No hallucinated Tesla information
- Explicit insufficient-evidence response
- Citation validation remains true

This demonstrates grounded refusal rather than hallucinated generation.

---

## Project Structure

---
invest-rag/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ processed/
â”‚   â”‚   â”œâ”€â”€ build_config.json
â”‚   â”‚   â”œâ”€â”€ chunks.jsonl
â”‚   â”‚   â””â”€â”€ chunks_manifest.json
â”‚   â””â”€â”€ samples/
â”‚       â””â”€â”€ sec_docs.jsonl
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ architecture.md
â”‚   â”œâ”€â”€ data_contract.md
â”‚   â””â”€â”€ evaluation.md
â”‚
â”œâ”€â”€ eval/
â”‚   â”œâ”€â”€ questions.jsonl
â”‚   â”œâ”€â”€ results_vector_doc.json
â”‚   â”œâ”€â”€ results_vector_chunk.json
â”‚   â”œâ”€â”€ results_rerank_llm_doc.json
â”‚   â”œâ”€â”€ results_rerank_llm_chunk.json
â”‚   â”œâ”€â”€ fails_vector_doc_k1.jsonl
â”‚   â”œâ”€â”€ fails_vector_doc_k10.jsonl
â”‚   â”œâ”€â”€ fails_vector_chunk_k1.jsonl
â”‚   â””â”€â”€ fails_vector_chunk_k10.jsonl
â”‚
â”œâ”€â”€ indexes/
â”‚   â””â”€â”€ faiss/
â”‚       â”œâ”€â”€ index.bin
â”‚       â”œâ”€â”€ meta.jsonl
â”‚       â”œâ”€â”€ build_config.json
â”‚       â””â”€â”€ chunks_manifest.json
â”‚
â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ retrieval_debug.jsonl
â”‚   â””â”€â”€ run_logs.jsonl
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 00_setup_and_ingest.ipynb
â”‚   â”œâ”€â”€ 01_build_vector_index.ipynb
â”‚   â”œâ”€â”€ 02_evaluate_retrieval.ipynb
â”‚   â””â”€â”€ 03_rag_generate.ipynb
â”‚
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ init_project.py
â”‚
â””â”€â”€ src/
    â”œâ”€â”€ config.py
    â”œâ”€â”€ __init__.py
    â”‚
    â”œâ”€â”€ app/
    â”‚   â”œâ”€â”€ cli.py
    â”‚   â”œâ”€â”€ rag_runtime.py
    â”‚   â””â”€â”€ __init__.py
    â”‚
    â”œâ”€â”€ data_pipeline/
    â”‚   â”œâ”€â”€ io_utils.py
    â”‚   â””â”€â”€ __init__.py
    â”‚
    â”œâ”€â”€ retrieval/
    â”‚   â”œâ”€â”€ artifacts.py
    â”‚   â”œâ”€â”€ build_vector_index.py
    â”‚   â”œâ”€â”€ chunk_loader.py
    â”‚   â”œâ”€â”€ vector_store.py
    â”‚   â””â”€â”€ __init__.py
    â”‚
    â”œâ”€â”€ llm/
    â”‚   â”œâ”€â”€ embedding.py
    â”‚   â”œâ”€â”€ rerank.py
    â”‚   â”œâ”€â”€ generate.py
    â”‚   â”œâ”€â”€ context.py
    â”‚   â””â”€â”€ __init__.py
    â”‚
    â””â”€â”€ eval/
        â”œâ”€â”€ retrieval_eval.py
        â”œâ”€â”€ retrieval_logging.py
        â”œâ”€â”€ search_wrappers.py
        â””â”€â”€ __init__.py
---

## What This Demonstrates

- Deterministic FAISS-based similarity search
- Measurable retrieval benchmarking
- Semantic reranking improvements
- Document vs chunk-level evaluation
- Strict citation-grounded generation
- Safe handling of unsupported entities

This project is structured to resemble a production-ready RAG system rather than a notebook prototype.
