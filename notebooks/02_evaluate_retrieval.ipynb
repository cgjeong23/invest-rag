{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 02. Evaluate Retrieval (Invest RAG)\n",
        "\n",
        "Measure retrieval quality **before** adding generation:\n",
        "- **Recall@k**, **MRR@k**\n",
        "- Compare **Vector baseline** vs **LLM rerank (Top-1 promotion)**\n",
        "\n",
        "> This notebook is orchestration-only. All logic lives under `src/`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation Design\n",
        "\n",
        "- Corpus: 5 companies (10-K filings)\n",
        "- ~4,000 chunks\n",
        "- 40 evaluation questions\n",
        "- Labels:\n",
        "  - `gold_doc_ids` (section-level)\n",
        "  - `gold_chunk_ids` (chunk-level)\n",
        "\n",
        "We evaluate retrieval at two levels:\n",
        "- **Doc-level** (routing correctness)\n",
        "- **Chunk-level** (fine-grained grounding precision)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Environment Setup\n",
        "\n",
        "Resolve project root and load index / evaluation paths."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard library\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "# Third-party\n",
        "import pandas as pd\n",
        "\n",
        "# Local modules\n",
        "from src.data_pipeline.io_utils import read_jsonl\n",
        "from src.llm.embedding import embed_query\n",
        "from src.retrieval.vector_store import VectorStore\n",
        "from src.eval.search_wrappers import (\n",
        "    make_vectorstore_search_fn,\n",
        "    make_llm_rerank_search_fn,\n",
        ")\n",
        "from src.eval.retrieval_eval import run_eval_suite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PROJECT_ROOT: c:\\Users\\CG\\Desktop\\invest-rag\n"
          ]
        }
      ],
      "source": [
        "PROJECT_ROOT = Path.cwd()\n",
        "assert (PROJECT_ROOT / \"src\").exists(), (\n",
        "    f\"Run this notebook from project root (invest-rag/). Current cwd={PROJECT_ROOT}\"\n",
        ")\n",
        "\n",
        "INDEX_DIR  = PROJECT_ROOT / \"indexes\" / \"faiss\"\n",
        "INDEX_PATH = INDEX_DIR / \"index.bin\"\n",
        "META_PATH  = INDEX_DIR / \"meta.jsonl\"\n",
        "EVAL_PATH  = PROJECT_ROOT / \"eval\" / \"questions.jsonl\"\n",
        "\n",
        "print(\"PROJECT_ROOT:\", PROJECT_ROOT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Evaluation Data & Index\n",
        "\n",
        "Load:\n",
        "- evaluation questions\n",
        "- FAISS vector index\n",
        "- metadata mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "n_questions: 40\n"
          ]
        }
      ],
      "source": [
        "# Load data + index\n",
        "questions = read_jsonl(EVAL_PATH)\n",
        "vs = VectorStore.load(index_path=INDEX_PATH, meta_path=META_PATH)\n",
        "\n",
        "print(\"n_questions:\", len(questions))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build `search_fn` (injection)\n",
        "\n",
        "Evaluator controls cutoff **k** by calling `search_fn(query, k)`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Baseline: vector search\n",
        "vector_search_fn = make_vectorstore_search_fn(\n",
        "    vs,\n",
        "    embed_query=embed_query,\n",
        "    normalize=True,   # keep consistent with index build\n",
        ")\n",
        "\n",
        "# Rerank v1: vector candidates -> LLM chooses best -> promote to rank #1\n",
        "rerank_search_fn = make_llm_rerank_search_fn(\n",
        "    vector_search_fn,\n",
        "    k_vec=10,\n",
        "    rerank_model=\"gpt-4.1-mini\", \n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation Helpers\n",
        "\n",
        "Utility functions for:\n",
        "- switching between doc-level and chunk-level evaluation\n",
        "- normalizing question format for the evaluator\n",
        "- performing lightweight sanity checks on retrieval outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "KS_DOC   = (1, 3, 5, 10)\n",
        "KS_CHUNK = (1, 3, 5, 10)   \n",
        "\n",
        "\n",
        "def prepare_questions_for_level(questions, level: str):\n",
        "    if level == \"doc\":\n",
        "        questions_eval = []\n",
        "        for q in questions:\n",
        "            qq = dict(q)\n",
        "            qq[\"question\"] = q[\"query\"]   # <- alias added\n",
        "            questions_eval.append(qq)\n",
        "        return questions_eval, \"doc_id\", True\n",
        "\n",
        "    if level == \"chunk\":\n",
        "        questions_eval = []\n",
        "        for q in questions:\n",
        "            qq = dict(q)\n",
        "            qq[\"question\"] = q[\"query\"]  \n",
        "            qq[\"gold_doc_ids\"] = q.get(\"gold_chunk_ids\", [])\n",
        "            questions_eval.append(qq)\n",
        "        return questions_eval, \"chunk_id\", False\n",
        "\n",
        "    raise ValueError(f\"Unknown level: {level}\")\n",
        "\n",
        "\n",
        "def quick_sanity_check(search_fn, q_item, id_key: str, n=5):\n",
        "    # q_item uses standardized key: \"question\"\n",
        "    sample = search_fn(q_item[\"question\"], n)\n",
        "    got = [r.get(id_key, None) for r in sample]\n",
        "    print(f\"[Sanity] qid={q_item.get('qid')} | id_key={id_key} | sample_ids={got}\")\n",
        "    if any(x is None for x in got):\n",
        "        print(f\"⚠️ WARNING: Some results missing '{id_key}'. Eval may be broken.\")\n",
        "    return sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Evaluation (Doc / Chunk)\n",
        "\n",
        "Run the same evaluation protocol for both levels and save results to `eval/`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_level_suite(*, questions, level: str, ks, vector_search_fn, rerank_search_fn, out_dir: Path):\n",
        "    questions_eval, id_key, dedupe = prepare_questions_for_level(questions, level)\n",
        "\n",
        "    print(f\"[Eval] level={level} | id_key={id_key} | dedupe={dedupe} | n={len(questions_eval)} | ks={ks}\")\n",
        "\n",
        "    # --- sanity check \n",
        "    _ = quick_sanity_check(vector_search_fn, questions_eval[0], id_key=id_key, n=3)\n",
        "    _ = quick_sanity_check(rerank_search_fn, questions_eval[0], id_key=id_key, n=3)\n",
        "\n",
        "    out_vec = out_dir / f\"results_vector_{level}.json\"\n",
        "    out_rr  = out_dir / f\"results_rerank_llm_{level}.json\"\n",
        "\n",
        "    suite_vec = run_eval_suite(\n",
        "        questions_eval,\n",
        "        ks=ks,\n",
        "        search_fn=vector_search_fn,\n",
        "        out_path=out_vec,\n",
        "        id_key=id_key,\n",
        "        dedupe=dedupe,\n",
        "    )\n",
        "\n",
        "    suite_rr = run_eval_suite(\n",
        "        questions_eval,\n",
        "        ks=ks,\n",
        "        search_fn=rerank_search_fn,\n",
        "        out_path=out_rr,\n",
        "        id_key=id_key,\n",
        "        dedupe=dedupe,\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"level\": level,\n",
        "        \"id_key\": id_key,\n",
        "        \"dedupe\": dedupe,\n",
        "        \"ks\": ks,\n",
        "        \"suite_vec\": suite_vec,\n",
        "        \"suite_rr\": suite_rr,\n",
        "        \"out_vec\": str(out_vec),\n",
        "        \"out_rr\": str(out_rr),\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Failure Logs\n",
        "\n",
        "Save failed queries as JSONL for quick inspection:\n",
        "- k=10 → coverage failures\n",
        "- k=1 → ranking failures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_fail_cases(*, questions, level: str, k: int, search_fn, out_path: Path):\n",
        "    questions_eval, id_key, _dedupe = prepare_questions_for_level(questions, level)\n",
        "\n",
        "    fails = []\n",
        "    for q in questions_eval:\n",
        "        gold = set(map(str, q.get(\"gold_doc_ids\", [])))\n",
        "        if not gold:\n",
        "            continue\n",
        "\n",
        "        results = search_fn(q[\"question\"], k) \n",
        "        pred = [str(r.get(id_key, \"\")) for r in results if r.get(id_key) is not None]\n",
        "\n",
        "        if not any(p in gold for p in pred):\n",
        "            fails.append({\n",
        "                \"qid\": q.get(\"qid\"),\n",
        "                \"tier\": q.get(\"tier\"),\n",
        "                \"type\": q.get(\"type\"),\n",
        "                \"question\": q.get(\"question\"),\n",
        "                \"notes\": q.get(\"notes\"),\n",
        "                \"level\": level,\n",
        "                \"k\": k,\n",
        "                \"id_key\": id_key,\n",
        "                \"gold_ids\": list(gold)[:50],\n",
        "                \"pred_ids\": pred,\n",
        "                \"top_results_preview\": [\n",
        "                    {\n",
        "                        id_key: r.get(id_key),\n",
        "                        \"doc_id\": r.get(\"doc_id\"),\n",
        "                        \"chunk_id\": r.get(\"chunk_id\"),\n",
        "                        \"score\": r.get(\"score\"),\n",
        "                    }\n",
        "                    for r in results[:min(5, len(results))]\n",
        "                ],\n",
        "            })\n",
        "\n",
        "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with out_path.open(\"w\", encoding=\"utf-8\") as f:\n",
        "        for row in fails:\n",
        "            f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "    print(f\"[FailLog] saved={len(fails)} -> {out_path}\")\n",
        "    return fails"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Metric Comparison Table\n",
        "\n",
        "Summarize Recall@k and MRR@k for:\n",
        "\n",
        "- **Vector baseline**\n",
        "- **Vector + LLM rerank**\n",
        "\n",
        "We also report:\n",
        "- ΔRecall = Rerank − Vector\n",
        "- ΔMRR = Rerank − Vector\n",
        "\n",
        "To avoid re-running expensive LLM reranking, we load cached evaluation results from `eval/`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== DOC-LEVEL ===\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>k</th>\n",
              "      <th>recall_at_k_vec</th>\n",
              "      <th>mrr_at_k_vec</th>\n",
              "      <th>recall_at_k_rr</th>\n",
              "      <th>mrr_at_k_rr</th>\n",
              "      <th>ΔRecall</th>\n",
              "      <th>ΔMRR</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0.750</td>\n",
              "      <td>0.7500</td>\n",
              "      <td>0.875</td>\n",
              "      <td>0.8750</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.1250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>0.850</td>\n",
              "      <td>0.8000</td>\n",
              "      <td>0.925</td>\n",
              "      <td>0.9000</td>\n",
              "      <td>0.075</td>\n",
              "      <td>0.1000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>0.900</td>\n",
              "      <td>0.8188</td>\n",
              "      <td>0.950</td>\n",
              "      <td>0.9125</td>\n",
              "      <td>0.050</td>\n",
              "      <td>0.0938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10</td>\n",
              "      <td>0.975</td>\n",
              "      <td>0.8375</td>\n",
              "      <td>0.975</td>\n",
              "      <td>0.9208</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0833</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    k  recall_at_k_vec  mrr_at_k_vec  recall_at_k_rr  mrr_at_k_rr  ΔRecall  \\\n",
              "0   1            0.750        0.7500           0.875       0.8750    0.125   \n",
              "1   3            0.850        0.8000           0.925       0.9000    0.075   \n",
              "2   5            0.900        0.8188           0.950       0.9125    0.050   \n",
              "3  10            0.975        0.8375           0.975       0.9208    0.000   \n",
              "\n",
              "     ΔMRR  \n",
              "0  0.1250  \n",
              "1  0.1000  \n",
              "2  0.0938  \n",
              "3  0.0833  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== CHUNK-LEVEL ===\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>k</th>\n",
              "      <th>recall_at_k_vec</th>\n",
              "      <th>mrr_at_k_vec</th>\n",
              "      <th>recall_at_k_rr</th>\n",
              "      <th>mrr_at_k_rr</th>\n",
              "      <th>ΔRecall</th>\n",
              "      <th>ΔMRR</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0.150</td>\n",
              "      <td>0.1500</td>\n",
              "      <td>0.350</td>\n",
              "      <td>0.3500</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.2000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>0.425</td>\n",
              "      <td>0.2792</td>\n",
              "      <td>0.525</td>\n",
              "      <td>0.4375</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.1583</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>0.525</td>\n",
              "      <td>0.3017</td>\n",
              "      <td>0.575</td>\n",
              "      <td>0.4488</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.1471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10</td>\n",
              "      <td>0.625</td>\n",
              "      <td>0.3135</td>\n",
              "      <td>0.625</td>\n",
              "      <td>0.4544</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.1408</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    k  recall_at_k_vec  mrr_at_k_vec  recall_at_k_rr  mrr_at_k_rr  ΔRecall  \\\n",
              "0   1            0.150        0.1500           0.350       0.3500     0.20   \n",
              "1   3            0.425        0.2792           0.525       0.4375     0.10   \n",
              "2   5            0.525        0.3017           0.575       0.4488     0.05   \n",
              "3  10            0.625        0.3135           0.625       0.4544     0.00   \n",
              "\n",
              "     ΔMRR  \n",
              "0  0.2000  \n",
              "1  0.1583  \n",
              "2  0.1471  \n",
              "3  0.1408  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import display\n",
        "\n",
        "EVAL_DIR = PROJECT_ROOT / \"eval\"\n",
        "\n",
        "def load_json(path: Path) -> dict:\n",
        "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def suite_to_df(suite: dict, label: str):\n",
        "    rows = []\n",
        "    for k in suite[\"ks\"]:\n",
        "        r = suite[\"results\"][str(k)]\n",
        "        rows.append({\n",
        "            \"model\": label,\n",
        "            \"k\": int(k),\n",
        "            \"n\": r.get(\"n\"),\n",
        "            \"recall_at_k\": r.get(\"recall_at_k\"),\n",
        "            \"mrr_at_k\": r.get(\"mrr_at_k\"),\n",
        "            \"n_fail\": r.get(\"n_fail\"),\n",
        "        })\n",
        "    return pd.DataFrame(rows).sort_values([\"k\"])\n",
        "\n",
        "def compare_table(suite_vec: dict, suite_rr: dict):\n",
        "    dfv = suite_to_df(suite_vec, \"Vector\")\n",
        "    dfr = suite_to_df(suite_rr, \"Rerank\")\n",
        "\n",
        "    df = dfv.merge(dfr, on=\"k\", suffixes=(\"_vec\", \"_rr\"))\n",
        "    df[\"ΔRecall\"] = df[\"recall_at_k_rr\"] - df[\"recall_at_k_vec\"]\n",
        "    df[\"ΔMRR\"]    = df[\"mrr_at_k_rr\"] - df[\"mrr_at_k_vec\"]\n",
        "\n",
        "    out = df[[\n",
        "        \"k\",\n",
        "        \"recall_at_k_vec\", \"mrr_at_k_vec\",\n",
        "        \"recall_at_k_rr\",  \"mrr_at_k_rr\",\n",
        "        \"ΔRecall\", \"ΔMRR\"\n",
        "    ]].copy()\n",
        "\n",
        "    for c in [\"recall_at_k_vec\",\"mrr_at_k_vec\",\"recall_at_k_rr\",\"mrr_at_k_rr\",\"ΔRecall\",\"ΔMRR\"]:\n",
        "        out[c] = out[c].astype(float).round(4)\n",
        "\n",
        "    return out\n",
        "\n",
        "def load_and_display(level: str):\n",
        "    vec_path = EVAL_DIR / f\"results_vector_{level}.json\"\n",
        "    rr_path  = EVAL_DIR / f\"results_rerank_llm_{level}.json\"\n",
        "\n",
        "    if not vec_path.exists() or not rr_path.exists():\n",
        "        raise FileNotFoundError(\n",
        "            f\"Missing results for level='{level}'.\\n\"\n",
        "            f\"- {vec_path}\\n- {rr_path}\\n\"\n",
        "            \"Run the evaluation cell first.\"\n",
        "        )\n",
        "\n",
        "    suite_vec = load_json(vec_path)\n",
        "    suite_rr  = load_json(rr_path)\n",
        "\n",
        "    print(f\"\\n=== {level.upper()}-LEVEL ===\")\n",
        "    display(compare_table(suite_vec, suite_rr))\n",
        "\n",
        "# Show both\n",
        "load_and_display(\"doc\")\n",
        "load_and_display(\"chunk\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results Interpretation\n",
        "\n",
        "### Doc-level Retrieval\n",
        "\n",
        "At the document level, vector retrieval already achieves high coverage by k=10 (R@10 ≈ 0.98), indicating that relevant sections are almost always present in the candidate set.\n",
        "\n",
        "LLM reranking significantly improves:\n",
        "- **R@1** (0.75 → 0.875)\n",
        "- **MRR** across all k\n",
        "\n",
        "This suggests that the primary bottleneck at the doc level is **ranking quality rather than candidate generation**.  \n",
        "Reranking effectively promotes the correct document to the top when it is already within the candidate pool.\n",
        "\n",
        "---\n",
        "\n",
        "### Chunk-level Retrieval\n",
        "\n",
        "At the chunk level, recall remains substantially lower even at k=10 (R@10 ≈ 0.63), indicating incomplete candidate coverage.\n",
        "\n",
        "However, reranking produces strong improvements in:\n",
        "- **R@1** (0.15 → 0.35)\n",
        "- **MRR** across all k\n",
        "\n",
        "This shows that when the correct chunk is present in the candidate set, reranking significantly improves answer positioning.  \n",
        "The main bottleneck at the chunk level appears to be **candidate generation (coverage), not ranking**.\n",
        "\n",
        "---\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "- **Doc-level:** Candidate generation is strong; ranking refinement provides measurable gains.\n",
        "- **Chunk-level:** Coverage remains limited at k=10; increasing retrieval depth or improving chunking strategy may further improve performance.\n",
        "- LLM reranking consistently improves early precision (R@1) and ranking quality (MRR), even when Recall@k is unchanged."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Failure Analysis\n",
        "\n",
        "We inspect failure cases to distinguish between:\n",
        "\n",
        "- **Coverage failures** (k=10 miss): correct item not retrieved\n",
        "- **Ranking failures** (k=1 miss): correct item retrieved but not ranked first\n",
        "\n",
        "Representative examples are shown below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'question': \"Which types of workloads are explicitly listed as examples where the company's accelerated computing stack is used? Provide at least three from the text.\",\n",
              "  'level': 'chunk',\n",
              "  'k': 10,\n",
              "  'id_key': 'chunk_id',\n",
              "  'gold_ids': ['nvidia_2024_item_1_business_c02_085ed035dcb2'],\n",
              "  'pred_ids': ['nvidia_2024_item_1_business_c22_44d1d4a16ddb',\n",
              "   'nvidia_2024_item_1_business_c03_18f3bf09b826',\n",
              "   'nvidia_2024_item_1_business_c23_09f6a7d3148f',\n",
              "   'nvidia_2024_item_1_business_c81_5257a3ad726c',\n",
              "   'nvidia_2024_item_1_business_c50_393fc5c3a9ed',\n",
              "   'nvidia_2024_item_1_business_c35_494c72a9061d',\n",
              "   'nvidia_2024_item_1a_risk_factors_c20_91bcdbc9d8d1',\n",
              "   'nvidia_2024_item_1_business_c21_b61f1b73881b',\n",
              "   'nvidia_2024_item_1_business_c39_f414782c55e9',\n",
              "   'nvidia_2024_item_1_business_c38_112a1f913afe'],\n",
              "  'top_results_preview': [{'chunk_id': 'nvidia_2024_item_1_business_c22_44d1d4a16ddb',\n",
              "    'doc_id': 'nvidia_2024_item_1_business',\n",
              "    'score': 0.5790205001831055},\n",
              "   {'chunk_id': 'nvidia_2024_item_1_business_c03_18f3bf09b826',\n",
              "    'doc_id': 'nvidia_2024_item_1_business',\n",
              "    'score': 0.5612400770187378},\n",
              "   {'chunk_id': 'nvidia_2024_item_1_business_c23_09f6a7d3148f',\n",
              "    'doc_id': 'nvidia_2024_item_1_business',\n",
              "    'score': 0.5588544607162476},\n",
              "   {'chunk_id': 'nvidia_2024_item_1_business_c81_5257a3ad726c',\n",
              "    'doc_id': 'nvidia_2024_item_1_business',\n",
              "    'score': 0.5524291396141052},\n",
              "   {'chunk_id': 'nvidia_2024_item_1_business_c50_393fc5c3a9ed',\n",
              "    'doc_id': 'nvidia_2024_item_1_business',\n",
              "    'score': 0.5519527792930603}]},\n",
              " {'question': \"A company says its data-center-scale offerings can scale to 'tens of thousands' of what kind of servers interconnected to function as a single giant computer?\",\n",
              "  'level': 'chunk',\n",
              "  'k': 10,\n",
              "  'id_key': 'chunk_id',\n",
              "  'gold_ids': ['nvidia_2024_item_1_business_c03_18f3bf09b826'],\n",
              "  'pred_ids': ['nvidia_2024_item_1_business_c04_dd93e121539d',\n",
              "   'nvidia_2024_item_1_business_c42_85ca7fbe84b5',\n",
              "   'nvidia_2024_item_1_business_c43_657ab3ce1864',\n",
              "   'amd_2024_item_1_business_c126_007f6a493984',\n",
              "   'amd_2024_item_1_business_c43_9b8d03312c58',\n",
              "   'amd_2024_item_1_business_c127_c056d0f3555a',\n",
              "   'nvidia_2024_item_1_business_c38_112a1f913afe',\n",
              "   'amd_2024_item_1_business_c128_16df7fca6d6f',\n",
              "   'amd_2024_item_1_business_c125_e70db90a4628',\n",
              "   'nvidia_2024_item_1_business_c39_f414782c55e9'],\n",
              "  'top_results_preview': [{'chunk_id': 'nvidia_2024_item_1_business_c04_dd93e121539d',\n",
              "    'doc_id': 'nvidia_2024_item_1_business',\n",
              "    'score': 0.695521354675293},\n",
              "   {'chunk_id': 'nvidia_2024_item_1_business_c42_85ca7fbe84b5',\n",
              "    'doc_id': 'nvidia_2024_item_1_business',\n",
              "    'score': 0.660452663898468},\n",
              "   {'chunk_id': 'nvidia_2024_item_1_business_c43_657ab3ce1864',\n",
              "    'doc_id': 'nvidia_2024_item_1_business',\n",
              "    'score': 0.63420569896698},\n",
              "   {'chunk_id': 'amd_2024_item_1_business_c126_007f6a493984',\n",
              "    'doc_id': 'amd_2024_item_1_business',\n",
              "    'score': 0.6153644919395447},\n",
              "   {'chunk_id': 'amd_2024_item_1_business_c43_9b8d03312c58',\n",
              "    'doc_id': 'amd_2024_item_1_business',\n",
              "    'score': 0.5991671085357666}]},\n",
              " {'question': 'A company reports two operating segments abbreviated FoA and RL. What do these abbreviations stand for?',\n",
              "  'level': 'chunk',\n",
              "  'k': 10,\n",
              "  'id_key': 'chunk_id',\n",
              "  'gold_ids': ['meta_2024_item_1_business_c07_b4a95af9576c'],\n",
              "  'pred_ids': ['meta_2024_item_7_mda_c59_bd86bb25c476',\n",
              "   'meta_2024_item_1_business_c08_86b1073c4f90',\n",
              "   'meta_2024_item_7_mda_c16_818abad3429e',\n",
              "   'meta_2024_item_1_business_c11_6f1935cb8ccd',\n",
              "   'meta_2024_item_7_mda_c181_4f4f91829f5e',\n",
              "   'meta_2024_item_7_mda_c182_b72405537b30',\n",
              "   'meta_2024_item_1_business_c12_2462e95449c2',\n",
              "   'meta_2024_item_7_mda_c15_4b010c2120c7',\n",
              "   'meta_2024_item_7_mda_c135_c3cd78197111',\n",
              "   'meta_2024_item_7_mda_c60_b0cadf20f3c7'],\n",
              "  'top_results_preview': [{'chunk_id': 'meta_2024_item_7_mda_c59_bd86bb25c476',\n",
              "    'doc_id': 'meta_2024_item_7_mda',\n",
              "    'score': 0.6125922203063965},\n",
              "   {'chunk_id': 'meta_2024_item_1_business_c08_86b1073c4f90',\n",
              "    'doc_id': 'meta_2024_item_1_business',\n",
              "    'score': 0.6110743284225464},\n",
              "   {'chunk_id': 'meta_2024_item_7_mda_c16_818abad3429e',\n",
              "    'doc_id': 'meta_2024_item_7_mda',\n",
              "    'score': 0.6057252883911133},\n",
              "   {'chunk_id': 'meta_2024_item_1_business_c11_6f1935cb8ccd',\n",
              "    'doc_id': 'meta_2024_item_1_business',\n",
              "    'score': 0.5604119300842285},\n",
              "   {'chunk_id': 'meta_2024_item_7_mda_c181_4f4f91829f5e',\n",
              "    'doc_id': 'meta_2024_item_7_mda',\n",
              "    'score': 0.5531550645828247}]}]"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def preview_fail(path, n=3):\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        rows = [json.loads(line) for line in f]\n",
        "    return rows[:n]\n",
        "\n",
        "preview_fail(EVAL_DIR / \"fails_vector_chunk_k10.jsonl\", n=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Observation**\n",
        "\n",
        "This failure occurs at the chunk level despite retrieving the correct document.\n",
        "The model retrieves semantically related chunks within the same section, but not the exact gold chunk.\n",
        "\n",
        "This suggests:\n",
        "- The evidence is distributed across multiple nearby chunks.\n",
        "- Chunk boundary granularity may affect strict chunk-level recall.\n",
        "- Increasing retrieval depth or using larger overlapping chunks could improve coverage."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNnmIOueuzZ3uLVxRQJITLO",
      "mount_file_id": "14TmA6iy_NAIMC_KgbTpGHO6DSNWIRrtL",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (invest-rag)",
      "language": "python",
      "name": "invest-rag"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
