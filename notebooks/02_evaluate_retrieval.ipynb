{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 02. Evaluate Retrieval (Invest RAG)\n",
        "\n",
        "Measure retrieval quality **before** adding generation:\n",
        "- **Recall@k**, **MRR@k**\n",
        "- Compare **Vector baseline** vs **LLM rerank (Top-1 promotion)**\n",
        "\n",
        "> This notebook is orchestration-only. All logic lives under `src/`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PROJECT_ROOT: C:\\Users\\CG\\Desktop\\invest-rag\n",
            "INDEX_PATH  : C:\\Users\\CG\\Desktop\\invest-rag\\indexes\\faiss\\index.bin\n",
            "META_PATH   : C:\\Users\\CG\\Desktop\\invest-rag\\indexes\\faiss\\meta.jsonl\n",
            "EVAL_PATH   : C:\\Users\\CG\\Desktop\\invest-rag\\eval\\questions.jsonl\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Robust project root detection (repo root should contain /src)\n",
        "CWD = Path.cwd().resolve()\n",
        "PROJECT_ROOT = CWD if (CWD / \"src\").exists() else CWD.parent\n",
        "\n",
        "INDEX_DIR = PROJECT_ROOT / \"indexes\" / \"faiss\"\n",
        "INDEX_PATH = INDEX_DIR / \"index.bin\"\n",
        "META_PATH  = INDEX_DIR / \"meta.jsonl\"\n",
        "EVAL_PATH  = PROJECT_ROOT / \"eval\" / \"questions.jsonl\"\n",
        "\n",
        "assert INDEX_PATH.exists(), f\"Missing: {INDEX_PATH}\"\n",
        "assert META_PATH.exists(), f\"Missing: {META_PATH}\"\n",
        "assert EVAL_PATH.exists(), f\"Missing: {EVAL_PATH}\"\n",
        "\n",
        "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
        "print(\"INDEX_PATH  :\", INDEX_PATH)\n",
        "print(\"META_PATH   :\", META_PATH)\n",
        "print(\"EVAL_PATH   :\", EVAL_PATH)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "n_questions: 20\n"
          ]
        }
      ],
      "source": [
        "from src.data_pipeline.io_utils import read_jsonl\n",
        "from src.llm.embedding import embed_query\n",
        "from src.retrieval.vector_store import VectorStore\n",
        "from src.eval.search_wrappers import make_vectorstore_search_fn, make_llm_rerank_search_fn\n",
        "from src.eval.retrieval_eval import run_eval_suite\n",
        "\n",
        "# Load data + index\n",
        "questions = read_jsonl(EVAL_PATH)\n",
        "vs = VectorStore.load(index_path=INDEX_PATH, meta_path=META_PATH)\n",
        "\n",
        "print(\"n_questions:\", len(questions))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build `search_fn` (injection)\n",
        "\n",
        "Evaluator controls cutoff **k** by calling `search_fn(query, k)`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Baseline: vector search\n",
        "vector_search_fn = make_vectorstore_search_fn(\n",
        "    vs,\n",
        "    embed_query=embed_query,\n",
        "    normalize=True,   # keep consistent with index build\n",
        ")\n",
        "\n",
        "# Rerank v1: vector candidates -> LLM chooses best -> promote to rank #1\n",
        "rerank_search_fn = make_llm_rerank_search_fn(\n",
        "    vector_search_fn,\n",
        "    k_vec=10,\n",
        "    rerank_model=\"gpt-4.1-mini\",  # or None to use default inside reranker\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run evaluation (baseline vs rerank)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "({'ks': [1, 3, 5, 10],\n",
              "  'results': {'1': {'k': 1,\n",
              "    'n': 20,\n",
              "    'n_scored': 20,\n",
              "    'skipped_no_gold': 0,\n",
              "    'recall_at_k': 0.5416666666666667,\n",
              "    'mrr_at_k': 0.85,\n",
              "    'n_fail': 3},\n",
              "   '3': {'k': 3,\n",
              "    'n': 20,\n",
              "    'n_scored': 20,\n",
              "    'skipped_no_gold': 0,\n",
              "    'recall_at_k': 0.7083333333333333,\n",
              "    'mrr_at_k': 0.8916666666666666,\n",
              "    'n_fail': 1},\n",
              "   '5': {'k': 5,\n",
              "    'n': 20,\n",
              "    'n_scored': 20,\n",
              "    'skipped_no_gold': 0,\n",
              "    'recall_at_k': 0.85,\n",
              "    'mrr_at_k': 0.8916666666666666,\n",
              "    'n_fail': 1},\n",
              "   '10': {'k': 10,\n",
              "    'n': 20,\n",
              "    'n_scored': 20,\n",
              "    'skipped_no_gold': 0,\n",
              "    'recall_at_k': 0.925,\n",
              "    'mrr_at_k': 0.9,\n",
              "    'n_fail': 0}}},\n",
              " {'ks': [1, 3, 5, 10],\n",
              "  'results': {'1': {'k': 1,\n",
              "    'n': 20,\n",
              "    'n_scored': 20,\n",
              "    'skipped_no_gold': 0,\n",
              "    'recall_at_k': 0.6416666666666667,\n",
              "    'mrr_at_k': 1.0,\n",
              "    'n_fail': 0},\n",
              "   '3': {'k': 3,\n",
              "    'n': 20,\n",
              "    'n_scored': 20,\n",
              "    'skipped_no_gold': 0,\n",
              "    'recall_at_k': 0.8083333333333332,\n",
              "    'mrr_at_k': 1.0,\n",
              "    'n_fail': 0},\n",
              "   '5': {'k': 5,\n",
              "    'n': 20,\n",
              "    'n_scored': 20,\n",
              "    'skipped_no_gold': 0,\n",
              "    'recall_at_k': 0.9,\n",
              "    'mrr_at_k': 1.0,\n",
              "    'n_fail': 0},\n",
              "   '10': {'k': 10,\n",
              "    'n': 20,\n",
              "    'n_scored': 20,\n",
              "    'skipped_no_gold': 0,\n",
              "    'recall_at_k': 0.925,\n",
              "    'mrr_at_k': 1.0,\n",
              "    'n_fail': 0}}})"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "KS = (1, 3, 5, 10)\n",
        "\n",
        "suite_vec = run_eval_suite(\n",
        "    questions,\n",
        "    ks=KS,\n",
        "    search_fn=vector_search_fn,\n",
        "    out_path=PROJECT_ROOT / \"eval\" / \"results_vector.json\",\n",
        "    id_key=\"doc_id\",  # switch to \"chunk_id\" if labels/results are chunk-level\n",
        "    dedupe=True,      # doc-level eval: True, chunk-level eval: False\n",
        ")\n",
        "\n",
        "suite_rr = run_eval_suite(\n",
        "    questions,\n",
        "    ks=KS,\n",
        "    search_fn=rerank_search_fn,\n",
        "    out_path=PROJECT_ROOT / \"eval\" / \"results_rerank_llm.json\",\n",
        "    id_key=\"doc_id\",\n",
        "    dedupe=True,\n",
        ")\n",
        "\n",
        "suite_vec, suite_rr\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results\n",
        "\n",
        "Compact comparison table (and delta)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " k |     Vector (R@k / MRR) |     Rerank (R@k / MRR) | ΔRecall | ΔMRR\n",
            "------------------------------------------------------------------------------\n",
            " 1 | 0.5417 / 0.8500     | 0.6417 / 1.0000     | +0.1000 | +0.1500\n",
            " 3 | 0.7083 / 0.8917     | 0.8083 / 1.0000     | +0.1000 | +0.1083\n",
            " 5 | 0.8500 / 0.8917     | 0.9000 / 1.0000     | +0.0500 | +0.1083\n",
            "10 | 0.9250 / 0.9000     | 0.9250 / 1.0000     | +0.0000 | +0.1000\n"
          ]
        }
      ],
      "source": [
        "def _row(suite, k: int):\n",
        "    m = suite[\"results\"][str(k)]\n",
        "    return m[\"recall_at_k\"], m[\"mrr_at_k\"], m.get(\"n_fail\", None)\n",
        "\n",
        "def print_compare(suite_a, suite_b, ks=KS, name_a=\"Vector\", name_b=\"Rerank\"):\n",
        "    print(f\"{'k':>2} | {name_a:>10} (R@k / MRR) | {name_b:>10} (R@k / MRR) | ΔRecall | ΔMRR\")\n",
        "    print(\"-\"*78)\n",
        "    for k in ks:\n",
        "        ra, ma, _ = _row(suite_a, k)\n",
        "        rb, mb, _ = _row(suite_b, k)\n",
        "        print(f\"{k:>2} | {ra:>6.4f} / {ma:>6.4f}     | {rb:>6.4f} / {mb:>6.4f}     | {rb-ra:+.4f} | {mb-ma:+.4f}\")\n",
        "\n",
        "print_compare(suite_vec, suite_rr)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results — Standard Set\n",
        "\n",
        "Reranking consistently improves top-ranked retrieval quality.\n",
        "\n",
        "- **Recall@1** improves by +10pp (0.54 → 0.64).\n",
        "- Gains diminish as k increases, indicating that reranking mainly refines ranking rather than expanding coverage.\n",
        "- MRR is relatively high because evaluation is performed at the document level (`doc_id`), which is less strict than chunk-level evaluation.\n",
        "\n",
        "Note: MRR values are relatively high because evaluation is conducted at the document level (`doc_id`) rather than chunk level, which makes the metric less strict."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hard Questions (Stress Test)\n",
        "\n",
        "We evaluate the same retrieval pipeline on a harder query set\n",
        "to test robustness under increased ambiguity and reduced lexical overlap.\n",
        "\n",
        "Same setup:\n",
        "- Same FAISS index\n",
        "- Same search functions (vector / rerank)\n",
        "- Same evaluator (Recall@k, MRR)\n",
        "\n",
        "Only the question set changes:\n",
        "`questions.jsonl` → `questions_hard.jsonl`\n",
        "\n",
        "We expect:\n",
        "- Overall performance to drop\n",
        "- Reranking to mainly improve top-ranked precision (Recall@1 / MRR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "n_questions: 20\n",
            " k |     Vector (R@k / MRR) |     Rerank (R@k / MRR) | ΔRecall | ΔMRR\n",
            "------------------------------------------------------------------------------\n",
            " 1 | 0.6917 / 0.9500     | 0.7417 / 1.0000     | +0.0500 | +0.0500\n",
            " 3 | 0.9083 / 0.9667     | 0.9333 / 1.0000     | +0.0250 | +0.0333\n",
            " 5 | 0.9750 / 0.9667     | 0.9750 / 1.0000     | +0.0000 | +0.0333\n",
            "10 | 0.9750 / 0.9667     | 0.9750 / 1.0000     | +0.0000 | +0.0333\n"
          ]
        }
      ],
      "source": [
        "EVAL_PATH  = PROJECT_ROOT / \"eval\" / \"questions_hard.jsonl\"\n",
        "\n",
        "questions_hard = read_jsonl(EVAL_PATH)\n",
        "print(\"n_questions:\", len(questions_hard))\n",
        "\n",
        "KS = (1, 3, 5, 10)\n",
        "\n",
        "suite_vec_hard = run_eval_suite(\n",
        "    questions_hard,\n",
        "    ks=KS,\n",
        "    search_fn=vector_search_fn,\n",
        "    out_path=PROJECT_ROOT / \"eval\" / \"results_vector_hard.json\",\n",
        "    id_key=\"doc_id\",  # switch to \"chunk_id\" if labels/results are chunk-level\n",
        "    dedupe=True,      # doc-level eval: True, chunk-level eval: False\n",
        ")\n",
        "\n",
        "suite_rr_hard = run_eval_suite(\n",
        "    questions_hard,\n",
        "    ks=KS,\n",
        "    search_fn=rerank_search_fn,\n",
        "    out_path=PROJECT_ROOT / \"eval\" / \"results_rerank_llm_hard.json\",\n",
        "    id_key=\"doc_id\",\n",
        "    dedupe=True,\n",
        ")\n",
        "\n",
        "print_compare(suite_vec_hard, suite_rr_hard)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results — Hard Set (Stress Test)\n",
        "\n",
        "On the hard query set (n=20), retrieval performance remains strong.\n",
        "\n",
        "Interestingly, Recall@1 is higher than on the standard set.\n",
        "This suggests that embedding-based retrieval may favor queries that closely\n",
        "resemble document phrasing, even if they appear conceptually more complex.\n",
        "\n",
        "Because evaluation is conducted at the document level (`doc_id`),\n",
        "matching any relevant document is sufficient for success,\n",
        "which may further amplify this effect.\n",
        "\n",
        "Reranking continues to improve top-1 precision,\n",
        "while gains diminish at larger k — indicating that reranking primarily\n",
        "refines ranking rather than expanding coverage.\n",
        "\n",
        "All experiments are conducted at the document level (`doc_id`).\n",
        "Chunk-level evaluation may provide a stricter and more fine-grained assessment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Failure Analysis (k=1)\n",
        "\n",
        "We inspect k=1 failures to understand where the retriever struggles.\n",
        "\n",
        "Representative failure patterns:\n",
        "\n",
        "1. **Lexical Mismatch**  \n",
        "   The query uses different terminology from the document, reducing embedding similarity.\n",
        "\n",
        "2. **Ambiguity / Underspecified Query**  \n",
        "   The query is broad, leading the retriever to select a plausible but incorrect document.\n",
        "\n",
        "3. **Ranking Limitation (Doc-level Evaluation)**  \n",
        "   The correct document may appear within top-k, but not at rank 1.\n",
        "\n",
        "These observations suggest potential improvements in query rewriting,\n",
        "more expressive reranking, and chunk-level evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " k |     Vector (R@k / MRR) |     Rerank (R@k / MRR) | ΔRecall | ΔMRR\n",
            "------------------------------------------------------------------------------\n",
            " 1 | 0.5417 / 0.8500     | 0.6417 / 1.0000     | +0.1000 | +0.1500\n",
            " 3 | 0.7083 / 0.8917     | 0.8083 / 1.0000     | +0.1000 | +0.1083\n",
            " 5 | 0.8500 / 0.8917     | 0.9000 / 1.0000     | +0.0500 | +0.1083\n",
            "10 | 0.9250 / 0.9000     | 0.9250 / 1.0000     | +0.0000 | +0.1000\n"
          ]
        }
      ],
      "source": [
        "KS = (1, 3, 5, 10)\n",
        "\n",
        "suite_vec = run_eval_suite(\n",
        "    questions,\n",
        "    ks=KS,\n",
        "    search_fn=vector_search_fn,\n",
        "    id_key=\"doc_id\",\n",
        "    dedupe=True,\n",
        ")\n",
        "\n",
        "suite_rr = run_eval_suite(\n",
        "    questions,\n",
        "    ks=KS,\n",
        "    search_fn=rerank_search_fn,\n",
        "    id_key=\"doc_id\",\n",
        "    dedupe=True,\n",
        ")\n",
        "\n",
        "print_compare(suite_vec, suite_rr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " k |     Vector (R@k / MRR) |     Rerank (R@k / MRR) | ΔRecall | ΔMRR\n",
            "------------------------------------------------------------------------------\n",
            " 1 | 0.6917 / 0.9500     | 0.7417 / 1.0000     | +0.0500 | +0.0500\n",
            " 3 | 0.9083 / 0.9667     | 0.9333 / 1.0000     | +0.0250 | +0.0333\n",
            " 5 | 0.9750 / 0.9667     | 0.9750 / 1.0000     | +0.0000 | +0.0333\n",
            "10 | 0.9750 / 0.9667     | 0.9750 / 1.0000     | +0.0000 | +0.0333\n"
          ]
        }
      ],
      "source": [
        "suite_vec_hard = run_eval_suite(\n",
        "    questions_hard,\n",
        "    ks=KS,\n",
        "    search_fn=vector_search_fn,\n",
        "    out_path=PROJECT_ROOT / \"eval\" / \"results_vector_hard.json\",\n",
        "    id_key=\"doc_id\",\n",
        "    dedupe=True,\n",
        ")\n",
        "\n",
        "suite_rr_hard = run_eval_suite(\n",
        "    questions_hard,\n",
        "    ks=KS,\n",
        "    search_fn=rerank_search_fn,\n",
        "    out_path=PROJECT_ROOT / \"eval\" / \"results_rerank_hard.json\",\n",
        "    id_key=\"doc_id\",\n",
        "    dedupe=True,\n",
        ")\n",
        "\n",
        "print_compare(suite_vec_hard, suite_rr_hard)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[standard] saved fail logs (k=1)\n",
            " - C:\\Users\\CG\\Desktop\\invest-rag\\eval\\results\\fails_vector_standard_k1.jsonl\n",
            " - C:\\Users\\CG\\Desktop\\invest-rag\\eval\\results\\fails_rerank_standard_k1.jsonl\n",
            " vector: Recall@1=0.5417, MRR@1=0.8500, n_fail=3\n",
            " rerank: Recall@1=0.6417, MRR@1=1.0000, n_fail=0\n",
            "[hard] saved fail logs (k=1)\n",
            " - C:\\Users\\CG\\Desktop\\invest-rag\\eval\\results\\fails_vector_hard_k1.jsonl\n",
            " - C:\\Users\\CG\\Desktop\\invest-rag\\eval\\results\\fails_rerank_hard_k1.jsonl\n",
            " vector: Recall@1=0.6917, MRR@1=0.9500, n_fail=1\n",
            " rerank: Recall@1=0.7417, MRR@1=1.0000, n_fail=0\n"
          ]
        }
      ],
      "source": [
        "from src.eval.retrieval_eval import evaluate_retrieval\n",
        "\n",
        "RESULTS_DIR = PROJECT_ROOT / \"eval\" / \"results\"\n",
        "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def save_fails_k1(questions, tag: str):\n",
        "    m_vec, _ = evaluate_retrieval(\n",
        "        questions,\n",
        "        k=1,\n",
        "        search_fn=vector_search_fn,\n",
        "        id_key=\"doc_id\",\n",
        "        dedupe=True,\n",
        "        save_fail_path=RESULTS_DIR / f\"fails_vector_{tag}_k1.jsonl\",\n",
        "    )\n",
        "    m_rr, _ = evaluate_retrieval(\n",
        "        questions,\n",
        "        k=1,\n",
        "        search_fn=rerank_search_fn,\n",
        "        id_key=\"doc_id\",\n",
        "        dedupe=True,\n",
        "        save_fail_path=RESULTS_DIR / f\"fails_rerank_{tag}_k1.jsonl\",\n",
        "    )\n",
        "\n",
        "    print(f\"[{tag}] saved fail logs (k=1)\")\n",
        "    print(\" -\", RESULTS_DIR / f\"fails_vector_{tag}_k1.jsonl\")\n",
        "    print(\" -\", RESULTS_DIR / f\"fails_rerank_{tag}_k1.jsonl\")\n",
        "    print(\" vector:\", f\"Recall@1={m_vec['recall_at_k']:.4f}, MRR@1={m_vec['mrr_at_k']:.4f}, n_fail={m_vec['n_fail']}\")\n",
        "    print(\" rerank:\", f\"Recall@1={m_rr['recall_at_k']:.4f}, MRR@1={m_rr['mrr_at_k']:.4f}, n_fail={m_rr['n_fail']}\")\n",
        "\n",
        "save_fails_k1(questions, tag=\"standard\")\n",
        "save_fails_k1(questions_hard, tag=\"hard\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "n_fails: 3\n",
            "--------------------------------------------------------------------------------\n",
            "Query: NSIL 3Q 출하와 마진에 영향을 주는 핵심 변수는?\n",
            "Gold: ['news_0001', 'report_0001']\n",
            "Top1: {'rank': 1, 'score': 0.47367626428604126, 'chunk_id': 'disc_0004_c00_00ae883d7406', 'doc_id': 'disc_0004', 'source': 'disclosure_note', 'date': '2025-10-02', 'company': 'NeuroSilicon', 'ticker': 'NSIL', 'sector': 'Semiconductor/AI', 'title': 'R&D 투자 확대 계획(요약)', 'tags': ['R&D', 'investment'], 'text': 'NSIL은 차세대 아키텍처 개발을 위해 R&D 투자를 확대할 계획을 공시. 2026년까지 단계적으로 집행될 예정이며 세부 항목은 추후 공개.'}\n",
            "--------------------------------------------------------------------------------\n",
            "Query: QuantaMemory HBM 증설 관련 투자 부담은?\n",
            "Gold: ['report_0006', 'news_0005']\n",
            "Top1: {'rank': 1, 'score': 0.5867177248001099, 'chunk_id': 'disc_0002_c00_e61a698b5841', 'doc_id': 'disc_0002', 'source': 'disclosure_note', 'date': '2025-09-08', 'company': 'QuantaMemory', 'ticker': 'QMEM', 'sector': 'Semiconductor/AI', 'title': 'HBM 생산라인 증설 계획(요약)', 'tags': ['HBM', 'capacity'], 'text': 'QMEM은 HBM 생산라인 증설을 발표. 단계적 증설로 2026년 상반기부터 순차 가동 예정. 투자 규모는 추후 확정 공시 예정.'}\n",
            "--------------------------------------------------------------------------------\n",
            "Query: AsterFoundry 선단 공정 프리미엄 유지 가능성은?\n",
            "Gold: ['report_0005']\n",
            "Top1: {'rank': 1, 'score': 0.44334644079208374, 'chunk_id': 'news_0003_c00_acef0834933e', 'doc_id': 'news_0003', 'source': 'news_summary', 'date': '2025-08-19', 'company': 'AsterFoundry', 'ticker': 'ASTF', 'sector': 'Semiconductor/AI', 'title': '선단 공정 수율 개선, 2H 가동률 회복 기대', 'tags': ['foundry', 'yield', 'utilization'], 'text': 'ASTF의 선단 공정 수율이 개선되며 고객사의 테이프아웃이 재개되는 흐름. 2H에는 가동률 회복이 기대되나, AI향 수요 집중으로 제품 믹스 편중 리스크가 존재.'}\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from src.data_pipeline.io_utils import read_jsonl\n",
        "\n",
        "fails = read_jsonl(PROJECT_ROOT / \"eval/results/fails_vector_standard_k1.jsonl\")\n",
        "\n",
        "print(\"n_fails:\", len(fails))\n",
        "print(\"-\" * 80)\n",
        "\n",
        "for r in fails[:3]:\n",
        "    print(\"Query:\", r[\"query\"])\n",
        "    print(\"Gold:\", r.get(\"gold_ids\") or r.get(\"gold_doc_ids\"))\n",
        "    print(\"Top1:\", r[\"top5\"][0])\n",
        "    print(\"-\" * 80)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNnmIOueuzZ3uLVxRQJITLO",
      "mount_file_id": "14TmA6iy_NAIMC_KgbTpGHO6DSNWIRrtL",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (invest-rag)",
      "language": "python",
      "name": "invest-rag"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
