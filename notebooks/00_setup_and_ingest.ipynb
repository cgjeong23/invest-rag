{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vjwT-n4oWUtt"
   },
   "source": [
    "# Invest-RAG\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Financial questions are high-stakes:\n",
    "LLM hallucinations can mislead investment decisions.\n",
    "\n",
    "This project builds a Retrieval-Augmented Generation system\n",
    "that grounds answers in financial documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All notebooks must be executed from the project root (invest-rag/).\n",
    "The project uses editable install mode (`pip install -e .`) to resolve imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: c:\\Users\\CG\\Desktop\\invest-rag\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "assert (PROJECT_ROOT / \"src\").exists(), (\n",
    "    f\"Run this notebook from the project root (invest-rag/). Current cwd={PROJECT_ROOT}\"\n",
    ")\n",
    "\n",
    "print(\"Project root:\", PROJECT_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GtJXk0CCWVgM"
   },
   "source": [
    "# 00. Setup & Ingest\n",
    "\n",
    "## Goal\n",
    "Build a reproducible data pipeline that prepares documents for RAG:\n",
    "raw docs â†’ cleaned text â†’ sentence split â†’ chunks (+ metadata)\n",
    "\n",
    "## Why this step matters\n",
    "Most RAG failures come from data issues:\n",
    "- inconsistent schema\n",
    "- missing metadata\n",
    "- noisy or duplicated text\n",
    "\n",
    "A clean ingestion pipeline improves downstream retrieval and evaluation.\n",
    "\n",
    "## Pipeline Overview\n",
    "1) Load raw documents  \n",
    "2) Validate schema (data contract)  \n",
    "3) Clean + normalize text  \n",
    "4) Sentence split + chunking  \n",
    "5) Save chunk dataset + manifests for reproducibility\n",
    "\n",
    "## Outputs (Artifacts)\n",
    "- `data/processed/chunks.jsonl` : chunk records with metadata\n",
    "- `data/processed/chunks_manifest.json` : summary stats + provenance\n",
    "- `data/processed/build_config.json` : pipeline configuration used in this run\n",
    "\n",
    "## Checkpoints\n",
    "- #docs ingested\n",
    "- #chunks generated\n",
    "- avg chunk length\n",
    "- 1â€“2 sample chunks preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 46,
     "status": "ok",
     "timestamp": 1771479525873,
     "user": {
      "displayName": "ì •ì±„ê°•",
      "userId": "01859992838092586619"
     },
     "user_tz": -540
    },
    "id": "eaTH6JGgZz_e",
    "outputId": "40959ec2-e706-4f22-b8bd-ece127f26a1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Project initialized at: c:\\Users\\CG\\Desktop\\invest-rag\n"
     ]
    }
   ],
   "source": [
    "from scripts.init_project import make_project\n",
    "\n",
    "make_project(PROJECT_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1j0i7SR4jUvh"
   },
   "source": [
    "## 1. Define a Data Contract  \n",
    "Consistency of data format is critical for a RAG pipeline.  \n",
    "This cell documents the schema for input documents (`news_summary.jsonl / report_excerpt.jsonl / disclosure_note.jsonl`) so that chunking, embedding, and evaluation follow the same contract.\n",
    "\n",
    "A clear data contract:\n",
    "- prevents silent bugs\n",
    "- makes evaluation fair\n",
    "- allows scaling to larger datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5u13W36jr_1"
   },
   "source": [
    "# Ingest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1k7okBQDMNI4"
   },
   "source": [
    "## 1. Cleaning + chunking  \n",
    "Chunking is one of the most impactful factors in RAG quality.\n",
    "\n",
    "Poor chunking â†’ poor retrieval â†’ hallucinated answers.\n",
    "\n",
    "Embedding long documents directly can hurt retrieval quality, so we split them into manageable chunks.  \n",
    "This cell applies light cleaning (whitespace/header removal), chunks by sentence grouping, and generates reproducible `chunk_id`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1771480359414,
     "user": {
      "displayName": "ì •ì±„ê°•",
      "userId": "01859992838092586619"
     },
     "user_tz": -540
    },
    "id": "MjLXwXTAf5I6",
    "outputId": "6a8778fa-5ddc-43c9-f465-9a59dd25d4dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 4297\n",
      "Example chunk keys: dict_keys(['chunk_id', 'chunk_index', 'text', 'metadata'])\n",
      "Example chunk_id: nvidia_2024_item_1_business_c00_937b0a34d837\n"
     ]
    }
   ],
   "source": [
    "import re, hashlib\n",
    "import json\n",
    "\n",
    "HEADER_PATTERNS = [\n",
    "    r\"^\\s*ìš”ì•½\\s*[:ï¼š]\\s*\",\n",
    "    r\"^\\s*Summary\\s*[:ï¼š]\\s*\",\n",
    "    r\"^\\s*í•µì‹¬\\s*[:ï¼š]\\s*\",\n",
    "]\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    t = text.strip()\n",
    "    t = t.replace(\"\\u00a0\", \" \")\n",
    "    t = re.sub(r\"[ \\t]+\", \" \", t)\n",
    "    t = re.sub(r\"\\n{3,}\", \"\\n\\n\", t)\n",
    "    for pat in HEADER_PATTERNS:\n",
    "        t = re.sub(pat, \"\", t)\n",
    "    return t.strip()\n",
    "\n",
    "def split_sentences(text: str):\n",
    "    '''\n",
    "    Sentence splitting (prototype baseline)\n",
    "\n",
    "    Sentence splitting uses a simple regex-based splitter as a lightweight baseline.\n",
    "    This is sufficient for the synthetic dataset, and can be swapped later with a more robust tokenizer/sentence segmenter if needed.\n",
    "    '''\n",
    "    # Simple sentence splitter for mixed Korean/English text \n",
    "    t = re.sub(r\"\\s*\\n\\s*\", \" \", text.strip())\n",
    "    if not t:\n",
    "        return []\n",
    "    # Sentence boundary candidates: \".\", \"!\", \"?\", and Korean ending \"ë‹¤.\"\n",
    "    parts = re.split(r\"(?<=[\\.\\!\\?])\\s+|(?<=ë‹¤\\.)\\s+\", t)\n",
    "    return [p.strip() for p in parts if p.strip()]\n",
    "\n",
    "def chunk_sentences(sents, max_chars=450, overlap_sents=1):\n",
    "    chunks = []\n",
    "    cur = []\n",
    "    cur_len = 0\n",
    "\n",
    "    def flush():\n",
    "        nonlocal cur, cur_len\n",
    "        if cur:\n",
    "            chunks.append(\" \".join(cur).strip())\n",
    "        cur = []\n",
    "        cur_len = 0\n",
    "\n",
    "    for s in sents:\n",
    "        if not cur:\n",
    "            cur = [s]\n",
    "            cur_len = len(s)\n",
    "            continue\n",
    "\n",
    "        if cur_len + 1 + len(s) <= max_chars:\n",
    "            cur.append(s)\n",
    "            cur_len += 1 + len(s)\n",
    "        else:\n",
    "            flush()\n",
    "            # Overlap strategy: append the last sentence of the previous chunk to the next chunk\n",
    "            if overlap_sents > 0 and chunks:\n",
    "                prev_sents = split_sentences(chunks[-1])\n",
    "                prefix = prev_sents[-overlap_sents:] if len(prev_sents) >= overlap_sents else prev_sents\n",
    "                cur = prefix + [s]\n",
    "                cur_len = sum(len(x) for x in cur) + (len(cur)-1)\n",
    "            else:\n",
    "                cur = [s]\n",
    "                cur_len = len(s)\n",
    "\n",
    "    flush()\n",
    "    return chunks\n",
    "\n",
    "def make_chunk_id(doc_id: str, chunk_index: int, chunk_text: str) -> str:\n",
    "    '''\n",
    "    Deterministic chunk IDs\n",
    "\n",
    "    Each `chunk_id` is generated deterministically as `sha1(doc_id | chunk_index | chunk_text)`,\n",
    "    so the same input documents produce identical chunk IDs across runs (useful for reproducible eval and debugging).\n",
    "\n",
    "    We use SHA-1 hashing to generate deterministic chunk IDs.\n",
    "    Given the same document and chunk text, the ID will always be identical. ensuring reproducibility across runs and experiments.\n",
    "    '''\n",
    "    h = hashlib.sha1(f\"{doc_id}|{chunk_index}|{chunk_text}\".encode(\"utf-8\")).hexdigest()[:12]\n",
    "    return f\"{doc_id}_c{chunk_index:02d}_{h}\"\n",
    "\n",
    "def doc_to_chunks(doc, max_chars=450, overlap_sents=1):\n",
    "    '''\n",
    "    Chunk size choice (max_chars=450, overlap_sents=1)\n",
    "\n",
    "    We use ~450 characters per chunk to balance retrieval granularity and context density:\n",
    "    smaller chunks improve precision but may lose key evidence; larger chunks improve recall but dilute similarity signals.\n",
    "    A 1-sentence overlap reduces boundary effects (facts split across chunks) with minimal duplication cost.\n",
    "    '''\n",
    "    content = clean_text(doc.get(\"content\", \"\"))\n",
    "    sents = split_sentences(content)\n",
    "\n",
    "    if len(content) <= max_chars:\n",
    "        chunk_texts = [content] if content else []\n",
    "    else:\n",
    "        chunk_texts = chunk_sentences(sents, max_chars=max_chars, overlap_sents=overlap_sents)\n",
    "\n",
    "    out = []\n",
    "    meta = {k: doc.get(k) for k in [\"doc_id\", \"company\",\"year\",\"section\",\"source\"]}\n",
    "    for i, ct in enumerate(chunk_texts):\n",
    "        out.append({\n",
    "            \"chunk_id\": make_chunk_id(doc[\"doc_id\"], i, ct),\n",
    "            \"chunk_index\": i,\n",
    "            \"text\": ct,\n",
    "            \"metadata\": meta\n",
    "        })\n",
    "    return out\n",
    "\n",
    "all_chunks = []\n",
    "#for d in docs:\n",
    "    #all_chunks.extend(doc_to_chunks(d, max_chars=450, overlap_sents=1))\n",
    "\n",
    "def load_jsonl(path: str):\n",
    "    docs = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                docs.append(json.loads(line))\n",
    "    return docs\n",
    "\n",
    "docs = load_jsonl(\"data/samples/sec_docs.jsonl\")\n",
    "\n",
    "for d in docs:\n",
    "    all_chunks.extend(doc_to_chunks(d, max_chars=450, overlap_sents=1))\n",
    "\n",
    "print(\"Total chunks:\", len(all_chunks))\n",
    "print(\"Example chunk keys:\", all_chunks[0].keys())\n",
    "print(\"Example chunk_id:\", all_chunks[0][\"chunk_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Y6VP26hN4K5"
   },
   "source": [
    "## 3. Sanity check & statistics\n",
    "We check chunk counts and length distribution to avoid extreme cases.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1771481720759,
     "user": {
      "displayName": "ì •ì±„ê°•",
      "userId": "01859992838092586619"
     },
     "user_tz": -540
    },
    "id": "bxhot-l6fzYp",
    "outputId": "05a3610c-8a83-444c-857d-a1a77130c88f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Chunk stats\n",
      "Chunks count: 4297\n",
      "Min/Median/Max length: 7 412 5228\n",
      "\n",
      "ðŸ“Š Chunks per doc (top 5):\n",
      "[('meta_2024_item_1a_risk_factors', 821), ('amd_2024_item_1a_risk_factors', 557), ('nvidia_2024_item_1a_risk_factors', 448), ('microsoft_2024_item_1a_risk_factors', 368), ('apple_2024_item_1a_risk_factors', 322)]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "CHUNKS_PATH = PROJECT_ROOT / \"data/processed/chunks.jsonl\"\n",
    "CHUNKS_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "lengths = [len(c[\"text\"]) for c in all_chunks]\n",
    "\n",
    "print(\"ðŸ“Š Chunk stats\")\n",
    "print(\"Chunks count:\", len(lengths))\n",
    "print(\n",
    "    \"Min/Median/Max length:\",\n",
    "    min(lengths),\n",
    "    sorted(lengths)[len(lengths)//2],\n",
    "    max(lengths)\n",
    ")\n",
    "\n",
    "cnt = Counter([c[\"metadata\"][\"doc_id\"] for c in all_chunks])\n",
    "print(\"\\nðŸ“Š Chunks per doc (top 5):\")\n",
    "print(cnt.most_common(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJ-aFrt_f5MP"
   },
   "source": [
    "## 4. Save & Summary\n",
    "We also inspect per-document chunk counts, then save the results to `chunks.jsonl`.\n",
    "\n",
    "Future work: evaluate chunk size and overlap hyperparameters using retrieval metrics (Recall@k, MRR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1771481725332,
     "user": {
      "displayName": "ì •ì±„ê°•",
      "userId": "01859992838092586619"
     },
     "user_tz": -540
    },
    "id": "MxW2ebBzifKo",
    "outputId": "2ffb4ef1-45d9-4fc9-8e5f-cc6c8a650051"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved chunks: c:\\Users\\CG\\Desktop\\invest-rag\\data\\processed\\chunks.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Save chunks.jsonl\n",
    "with CHUNKS_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for c in all_chunks:\n",
    "        f.write(json.dumps(c, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"âœ… Saved chunks:\", CHUNKS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32,
     "status": "ok",
     "timestamp": 1771481731028,
     "user": {
      "displayName": "ì •ì±„ê°•",
      "userId": "01859992838092586619"
     },
     "user_tz": -540
    },
    "id": "FuOGMeLogj_K",
    "outputId": "04639262-2797-4e67-da0a-1c530f5376d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved manifest: c:\\Users\\CG\\Desktop\\invest-rag\\data\\processed\\chunks_manifest.json\n",
      "âœ… Saved build config: c:\\Users\\CG\\Desktop\\invest-rag\\data\\processed\\build_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CG\\AppData\\Local\\Temp\\ipykernel_29252\\2556389700.py:7: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"created_at\": datetime.utcnow().isoformat() + \"Z\",\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "MANIFEST_PATH = PROJECT_ROOT / \"data/processed/chunks_manifest.json\"\n",
    "CONFIG_PATH   = PROJECT_ROOT / \"data/processed/build_config.json\"\n",
    "\n",
    "manifest = {\n",
    "    \"created_at\": datetime.utcnow().isoformat() + \"Z\",\n",
    "    \"n_docs\": len(docs),\n",
    "    \"n_chunks\": len(all_chunks),\n",
    "    \"chunk_max_chars\": 450,\n",
    "    \"overlap_sents\": 1,\n",
    "    \"inputs\": \"sec_docs.jsonl\",\n",
    "}\n",
    "\n",
    "build_config = {\n",
    "    \"cleaning\": {\"header_patterns\": HEADER_PATTERNS},\n",
    "    \"sentence_split\": \"regex-based (prototype)\",\n",
    "    \"chunking\": {\"max_chars\": 450, \"overlap_sents\": 1},\n",
    "    \"outputs\": {\n",
    "        \"chunks_jsonl\": str(CHUNKS_PATH),\n",
    "        \"manifest\": str(MANIFEST_PATH),\n",
    "        \"build_config\": str(CONFIG_PATH),\n",
    "    },\n",
    "}\n",
    "\n",
    "MANIFEST_PATH.write_text(json.dumps(manifest, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "CONFIG_PATH.write_text(json.dumps(build_config, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(\"âœ… Saved manifest:\", MANIFEST_PATH)\n",
    "print(\"âœ… Saved build config:\", CONFIG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 49,
     "status": "ok",
     "timestamp": 1771481767987,
     "user": {
      "displayName": "ì •ì±„ê°•",
      "userId": "01859992838092586619"
     },
     "user_tz": -540
    },
    "id": "CJWQXUVQippS",
    "outputId": "bf57e7d6-45bf-471a-caf9-931c348adbe5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Final Artifacts\n",
      "----------------------------------------\n",
      "chunks_jsonl    | exists=True | size=466302254 bytes | c:\\Users\\CG\\Desktop\\invest-rag\\data\\processed\\chunks.jsonl\n",
      "manifest        | exists=True | size=170 bytes | c:\\Users\\CG\\Desktop\\invest-rag\\data\\processed\\chunks_manifest.json\n",
      "build_config    | exists=True | size=585 bytes | c:\\Users\\CG\\Desktop\\invest-rag\\data\\processed\\build_config.json\n",
      "\n",
      "ðŸ§¾ Manifest Summary:\n",
      "  created_at: 2026-03-01T09:41:07.709914Z\n",
      "  n_docs: 15\n",
      "  n_chunks: 4297\n",
      "  chunk_max_chars: 450\n",
      "  overlap_sents: 1\n",
      "  inputs: sec_docs.jsonl\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "PROCESSED_DIR = PROJECT_ROOT / \"data/processed\"\n",
    "\n",
    "ARTIFACTS = {\n",
    "    \"chunks_jsonl\": PROCESSED_DIR / \"chunks.jsonl\",\n",
    "    \"manifest\": PROCESSED_DIR / \"chunks_manifest.json\",\n",
    "    \"build_config\": PROCESSED_DIR / \"build_config.json\",\n",
    "}\n",
    "\n",
    "print(\"ðŸ“¦ Final Artifacts\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for name, path in ARTIFACTS.items():\n",
    "    exists = path.exists()\n",
    "    size = path.stat().st_size if exists else 0\n",
    "    print(f\"{name:15} | exists={exists} | size={size} bytes | {path}\")\n",
    "\n",
    "# Optional: preview manifest\n",
    "if ARTIFACTS[\"manifest\"].exists():\n",
    "    manifest = json.loads(ARTIFACTS[\"manifest\"].read_text(encoding=\"utf-8\"))\n",
    "    print(\"\\nðŸ§¾ Manifest Summary:\")\n",
    "    for k, v in manifest.items():\n",
    "        print(f\"  {k}: {v}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOoVAKL5CJlm+80od1CqYWc",
   "mount_file_id": "1cQCTAyXtjXLVrKEkXnD_5--pW8yJP28V",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (invest-rag)",
   "language": "python",
   "name": "invest-rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
